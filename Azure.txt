Got it âœ…
Since youâ€™re using **`argo-githubactions-appset`** as the GitOps repo folder structure, Iâ€™ll prepare a **ready-to-use SecretProviderClass Helm template** that fits into your setup and is **easy to configure per environment**.

---

## ðŸ“‚ GitOps Folder Structure (with Key Vault support)

```
argo-githubactions-appset/
â”œâ”€ .github/workflows/
â”‚  â”œâ”€ update-gitops.yaml               # reusable workflow
â”‚  â”œâ”€ deploy-microservice-1.yaml
â”‚  â”œâ”€ deploy-microservice-2.yaml
â”œâ”€ gitops/
â”‚  â”œâ”€ argo-appset.yaml                 # ApplicationSet definition
â”‚  â”œâ”€ charts/app/                      # Reusable Helm chart
â”‚  â”‚  â”œâ”€ templates/
â”‚  â”‚  â”‚  â”œâ”€ deployment.yaml
â”‚  â”‚  â”‚  â”œâ”€ service.yaml
â”‚  â”‚  â”‚  â”œâ”€ secretproviderclass.yaml   # <-- CSI driver integration
â”‚  â”‚  â””â”€ values.yaml
â”‚  â””â”€ microservices/
â”‚     â””â”€ microservice-1/
â”‚        â”œâ”€ dev/values.yaml
â”‚        â”œâ”€ tst/values.yaml
â”‚        â”œâ”€ uat/values.yaml
â”‚        â””â”€ prod/values.yaml
```

---

## ðŸ›  `charts/app/templates/secretproviderclass.yaml`

```yaml
{{- if .Values.azureKeyVault.enabled }}
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: {{ include "app.fullname" . }}-akv
spec:
  provider: azure
  parameters:
    usePodIdentity: "false"  # if using managed identity set this true
    useVMManagedIdentity: "true"
    userAssignedIdentityID: {{ .Values.azureKeyVault.userAssignedIdentityID | quote }}
    keyvaultName: {{ .Values.azureKeyVault.name | quote }}
    cloudName: "" # AzureCloud, AzureUSGovernment, AzureChinaCloud
    objects: |
      array:
        - |
          objectName: {{ .Values.azureKeyVault.secretName }}
          objectType: secret
    tenantId: {{ .Values.azureKeyVault.tenantId | quote }}
{{- end }}
```

---

## âš™ï¸ Example `values.yaml` per Environment

### `gitops/microservices/microservice-1/dev/values.yaml`

```yaml
image:
  repository: myacr.azurecr.io/microservice-1
  tag: dev-latest

azureKeyVault:
  enabled: true
  name: my-dev-keyvault
  tenantId: "00000000-0000-0000-0000-000000000000"
  userAssignedIdentityID: "11111111-1111-1111-1111-111111111111"
  secretName: "db-connection-string"
```

### `gitops/microservices/microservice-1/prod/values.yaml`

```yaml
image:
  repository: myacr.azurecr.io/microservice-1
  tag: prod-latest

azureKeyVault:
  enabled: true
  name: my-prod-keyvault
  tenantId: "00000000-0000-0000-0000-000000000000"
  userAssignedIdentityID: "22222222-2222-2222-2222-222222222222"
  secretName: "db-connection-string"
```

---

âœ… Exactly â€” your understanding is **spot on**.
Right now your **SecretProviderClass** exists, but since you havenâ€™t added the **CSI volume mount in your Deployment**, ArgoCD syncs the resource, but the secrets never get projected into the pod â†’ which is why `/mnt/secrets-store/` is missing inside the container.

So yes, the **missing link** is in `deployment.yaml`.

---

### ðŸ”¹ How it ties into your GitOps structure

In your **Helm chart (`charts/app/templates/deployment.yaml`)**, you need to conditionally mount the secrets volume whenever `azureKeyVault.enabled=true`.

---

## ðŸ›  `charts/app/templates/deployment.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "app.fullname" . }}
  labels:
    app: {{ include "app.name" . }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ include "app.name" . }}
  template:
    metadata:
      labels:
        app: {{ include "app.name" . }}
    spec:
      containers:
        - name: {{ include "app.name" . }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
          {{- if .Values.azureKeyVault.enabled }}
          volumeMounts:
            - name: secrets-store
              mountPath: /mnt/secrets-store
              readOnly: true
          {{- end }}
      {{- if .Values.azureKeyVault.enabled }}
      volumes:
        - name: secrets-store
          csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: {{ include "app.fullname" . }}-akv
      {{- end }}
```

---

### ðŸ”¹ Now with your example

* When `azureKeyVault.enabled: true` in `values.yaml`, your pod gets:

  * `/mnt/secrets-store/` populated with Key Vault secrets as **files**
* When `azureKeyVault.enabled: false`, no CSI volume is injected â†’ so deployment works without Key Vault.

---

### ðŸ”¹ Optional: Sync to Kubernetes Secret

If you also want **ENV variables** support â†’ extend `secretproviderclass.yaml` with `secretObjects`. Then in `deployment.yaml`, you can map to `envFrom.secretRef`.

---

ðŸ‘‰ So yes:

* `secretproviderclass.yaml` = defines how to pull from Key Vault.
* `deployment.yaml` = actually mounts it into your pod.
* ArgoCD will sync both since theyâ€™re part of the same Helm chart.

---

Do you want me to extend your **Helm chart skeleton** so that it supports **both file mount + Kubernetes Secret sync** (so microservices can choose whichever way they want via `values.yaml`)?


on:
  workflow_dispatch:
    inputs:
      create_rg:
        description: "Create Resource Group (base infra container for all resources)"
        default: "true"
      create_keyvault:
        description: "Create Azure Key Vault (secrets management)"
        default: "true"
      create_postgres:
        description: "Create Azure Postgres Flexible Server (database)"
        default: "true"
      create_aks:
        description: "Create AKS Cluster (Kubernetes workloads)"
        default: "true"
      create_storage:
        description: "Create Azure Storage Account (blob/file storage)"
        default: "false"
      create_redis:
        description: "Create Azure Cache for Redis (caching layer)"
        default: "false"
      create_namespace:
        description: "Create Kubernetes namespaces (app + argocd namespaces)"
        default: "true"
      create_argocd:
        description: "Install ArgoCD (GitOps engine for deployments)"
        default: "true"
      create_pgadmin:
        description: "Deploy PGAdmin in AKS (Postgres management UI)"
        default: "false"
      environment:
        description: "Target environment (dev, tst, uat, prod)"
        default: "dev"
------------------------------------------------------------------------------------------------
provider "azurerm" {
  features {}
}

resource "azurerm_resource_group" "storage" {
  name     = var.rg_name
  location = var.location
}

resource "azurerm_storage_account" "tfstate" {
  name                     = var.storage_account_name
  resource_group_name      = azurerm_resource_group.storage.name
  location                 = azurerm_resource_group.storage.location
  account_tier             = "Standard"
  account_replication_type = "LRS"
  allow_blob_public_access = false
}

resource "azurerm_storage_container" "tfstate" {
  name                  = "tfstate"
  storage_account_name  = azurerm_storage_account.tfstate.name
  container_access_type = "private"
}

output "storage_account_name" {
  value = azurerm_storage_account.tfstate.name
}

output "container_name" {
  value = azurerm_storage_container.tfstate.name
}

output "resource_group_name" {
  value = azurerm_resource_group.storage.name
}

provider "azurerm" {
  features {}
}

resource "azurerm_resource_group" "storage" {
  name     = var.rg_name
  location = var.location
}

resource "azurerm_storage_account" "tfstate" {
  name                     = var.storage_account_name
  resource_group_name      = azurerm_resource_group.storage.name
  location                 = azurerm_resource_group.storage.location
  account_tier             = "Standard"
  account_replication_type = "LRS"
  allow_blob_public_access = false
}

resource "azurerm_storage_container" "tfstate" {
  name                  = "tfstate"
  storage_account_name  = azurerm_storage_account.tfstate.name
  container_access_type = "private"
}

output "storage_account_name" {
  value = azurerm_storage_account.tfstate.name
}

output "container_name" {
  value = azurerm_storage_container.tfstate.name
}

output "resource_group_name" {
  value = azurerm_resource_group.storage.name
}


output:
output "storage_account_name" {
  description = "Storage Account used for Terraform state"
  value       = azurerm_storage_account.tfstate.name
}

output "container_name" {
  description = "Container used for Terraform state"
  value       = azurerm_storage_container.tfstate.name
}

output "resource_group_name" {
  description = "Resource Group containing the storage account"
  value       = azurerm_resource_group.storage.name
}



